К сожалению, я не могу создавать или отправлять файлы. Однако я могу предоставить ответы на все ваши вопросы в структурированном текстовом формате прямо здесь. Вы можете легко скопировать этот текст и вставить его в файл (например, `.txt` или `.md`).

Вот подробные ответы:

---

### **РАЗДЕЛ 1: ПАРАДИГМЫ SOFTWARE**

**1. Что такое Software 1.0?**
Это классическое программирование, где разработчик вручную пишет каждую инструкцию (алгоритм) на языках вроде C++, Java, Python.
*   **a. Примеры:** Операционные системы (Linux), системы управления базами данных (PostgreSQL), веб-сервер (Apache), игры (Super Mario).
*   **b. Почему детерминирован?** Код выполняется строго по написанным инструкциям. При одинаковых входных данных всегда дает одинаковый результат.
*   **c. Главный компонент:** **Явный алгоритм / Логика, написанная программистом.**

**2. Что такое Software 2.0?**
Парадигма, где поведение программы определяется не жестким кодом, а **параметрами нейронной сети**, обученной на данных.
*   **a. Примеры:** Распознавание лиц, голосовые помощники (Siri), машинный перевод (DeepL), рекомендательные системы (YouTube, Netflix).
*   **b. Инструменты:** TensorFlow, PyTorch, Keras, CUDA.
*   **c. Почему не детерминирован?** Поведение зависит от данных, на которых обучалась модель, и случайных факторов при обучении (инициализация весов). Может выдавать разные результаты для схожих входов или быть непредсказуемым на новых данных.
*   **d. Главный компонент:** **Обученная модель (набор весов нейронной сети) и данные.**

**3. Что такое Software 3.0?**
Парадигма, где поведение программы определяется **инструкциями на естественном языке (промптами)**, выполняемыми большими языковыми моделями (LLM).
*   **a. Инструменты:** OpenAI GPT API, Anthropic Claude API, LangChain, LlamaIndex.
*   **b. Главный компонент:** **Промпт (инструкция) и LLM как "универсальный исполнитель".**
*   **c. Является ли предсказуемым?** **Нет.** LLM по своей природе стохастичны (зависит от температуры), могут "галлюцинировать" и их выход сложно гарантировать на 100%.

**4. Могут ли все парадигмы существовать вместе?**
**Да.** Они часто комбинируются в современных системах.
*   **a. Пример в одном приложении (веб-сервис):**
    *   **Software 1.0:** Бэкенд на Python (Django/Flask), обработка HTTP-запросов, бизнес-логика.
    *   **Software 2.0:** Модель для определения спама в комментариях пользователей.
    *   **Software 3.0:** Чат-бот поддержки на основе LLM, который отвечает на вопросы пользователей.
*   **b. Система контроля доступа:** **Software 1.0.** Это критичная для безопасности, простая, детерминированная логика (сравнение хэшей паролей, проверка токенов). Она должна быть абсолютно надежной и предсказуемой, что неприемлемо для вероятностных парадигм 2.0 и 3.0.

**5. Главное отличие Software 1.0 от Software 2.0?**
**Software 1.0** основан на **явной логике**, написанной человеком. **Software 2.0** основан на **скрытых закономерностях**, извлеченных из данных.
*   **a. Почему 1.0 плох для паттернов?** Человеку сложно формализовать и написать правила для сложных, нечетких паттернов (например, отличить кошку от собаки на всех фото).
*   **b. Почему 2.0 хороша для больших данных?** Нейросети автоматически находят сложные корреляции и зависимости в огромных объемах данных, которые человек не в состоянии обработать и выразить в коде.

**6. Какая парадигма лучше для web-приложений?**
**Software 1.0** (и его наследники, как 1.5 - с использованием библиотек и фреймворков). Веб-приложения требуют надежности, безопасности, точной логики рендеринга и работы с БД.
*   **a. Может ли 3.0 полностью заменить 1.0?** **Нет.** LLM непригодны для гарантированного выполнения точных, безопасных операций (обработка платежей, управление состоянием приложения). Они дополняют, а не заменяют.

**7. Когда использование Software 3.0 рискованно?**
В задачах, требующих **гарантированной точности, детерминизма и отсутствия галлюцинаций**: медицинские диагнозы, финансовые расчеты, управление критической инфраструктурой, выдача юридических заключений.
*   **a. Минус, не решаемый промптингом:** **Принципиальная недетерминированность и склонность к галлюцинациям.** Даже с идеальным промптом LLM может выдать некорректный, но убедительно звучащий ответ.

**8. Пример Software 2.0 и 3.0:**
*   **Software 2.0:** Система автодополнения кода в IDE (на основе модели, обученной на кодовой базе).
*   **Software 3.0:** Виртуальный ассистент, который по текстовому запросу "спланируй мне отпуск в Италии на май" находит рейсы, отели и составляет маршрут, общаясь с API.
*   **a. Почему 3.0 более универсален?** **Software 2.0** — это узкоспециализированная модель, обученная под конкретную задачу (распознавание изображений, перевод). **Software 3.0 (LLM)** — это "универсальный решатель", который одним промптом можно перенастроить на сотни разных задач без переобучения.

---
### **РАЗДЕЛ 2: GIT И GITVERSE**

**9. Что такое Git?**
Распределенная система контроля версий для отслеживания изменений в коде.
*   **a. Разница Git и GitHub:** Git — инструмент (программа), GitHub — веб-сервис (платформа) для хостинга Git-репозиториев и коллаборации.
*   **b. Что такое GitVerse?** Образовательная платформа/форк GitHub, созданная для учебных целей (вероятно, с интеграцией заданий и CI/CD).
*   **c. Отличие с точки зрения разработчика:** Интерфейс и дополнительные образовательные фичи (автопроверка заданий, специальные workflow). Основные команды Git (`commit`, `push`, `pull`) идентичны.

**10. Три состояния файла в Git:**
Modified (изменен), Staged (подготовлен к коммиту), Committed (зафиксирован в репозитории).
*   **a. `git add`:** Переводит файлы из Modified в Staged.
*   **b. `git commit`:** Фиксирует изменения из Staged, создавая snapshot (снимок) проекта.
*   **c. `git status`:** Показывает текущее состояние рабочей директории и staging area.

**11. Что такое ветка (branch)?**
Указатель на конкретный коммит. Позволяет вести параллельные линии разработки.
*   **a. Когда создать новую ветку?** Всегда, когда начинаете новую фичу, фикс или эксперимент. `main`/`master` должен оставаться стабильным.
*   **b. Что такое HEAD?** Ссылка, указывающая на текущий активный коммит (обычно на последний коммит в текущей ветке).

**12. Разница между `commit` и `push`:**
*   **`commit`** сохраняет изменения **локально** в вашем репозитории.
*   **`push`** отправляет ваши коммиты **на удаленный сервер** (GitHub/GitVerse).
*   **a. Rebase vs Merge:**
    *   **Merge:** Создает новый коммит-слияние, сохраняя историю обеих веток "как было".
    *   **Rebase:** "Перемещает" коммиты вашей ветки, ставя их на конец целевой ветки, создавая линейную историю.

**13. При merge изменений одной строки:**
Возникает **конфликт слияния (merge conflict)**.
*   **a. Что такое merge conflict?** Ситуация, когда Git не может автоматически решить, какую версию изменения использовать.
*   **b. Как разрешить?** Вручную отредактировать конфликтный файл (убрать маркеры `<<<<<<<`, `=======`, `>>>>>>>`), оставив нужный код, затем сделать `git add` и `git commit`.

**14. Настройка защиты ветки:**
В настройках репозитория на GitVerse (Settings -> Branches -> Branch protection rules).
*   **a. Создать ветку:** `git branch <name>` или `git checkout -b <name>` (создать и переключиться).
*   **b. Удаление веток:** Актуально после успешного мержа Pull Request'а, чтобы не захламлять список. `git branch -d <name>`.
*   **c. Переключиться:** `git checkout <branch-name>` или `git switch <branch-name>`.

**15. Что такое Pull Request (PR)?**
Запрос на слияние изменений из одной ветки в другую (часто в `main`). Механизм для **code review** и обсуждения кода.
*   **a. Почему важен?** Позволяет команде проверять код, находить баги, делиться знаниями, поддерживать качество и стандарты.
*   **b. Кто ревьюер?** Другие разработчики команды. **Важно разнообразие:** Senior dev ловит архитектурные ошибки, коллеги по команде понимают контекст, newcomers приносят свежий взгляд.

**16. Что такое `.gitignore`?**
Файл, в котором перечислены шаблоны файлов/папок, которые Git должен игнорировать (не добавлять в репозиторий).
*   **a. Типичные файлы:** `node_modules/`, `__pycache__/`, `.env`, `*.log`, `*.tmp`, файлы IDE (`.idea/`, `.vscode/`), бинарные файлы.

---
### **РАЗДЕЛ 3: CI/CD И YAML**

**17. Зачем CI/CD?**
Для автоматизации сборки, тестирования и развертывания приложений. Увеличивает скорость, надежность и частоту релизов.
*   **a. CI (Continuous Integration):** Непрерывная интеграция — автоматическая сборка и тестирование каждого коммита.
*   **b. CD (Continuous Delivery/Deployment):** Непрерывная доставка/развертывание — автоматический деплой готового кода в среду.
*   **c. Инструменты в семестре:** **GitHub/GitVerse Actions.**

**18. Основные блоки YAML workflow:**
`name`, `on` (триггеры), `jobs` (задачи, содержащие `runs-on`, `steps`).
*   **a. Где хранится?** В папке `.github/workflows/` репозитория.
*   **b. Название для домашек:** Обычно `classroom.yml` или подобное, заданное инструкцией.

**19. События для workflow:**
`push`, `pull_request`, `schedule`, `workflow_dispatch` (ручной запуск).
*   **a. Job:** Набор `steps`, которые выполняются на одном `runner`. Шаги могут зависеть друг от друга.
*   **b. Step:** Отдельная задача в job (например, запуск команды, использование action).

**20. Что такое runner?**
Сервер (виртуальная машина или контейнер), который выполняет workflow.
*   **a. `runs-on: ubuntu-latest`:** Указывает, что job должен запуститься на последней версии Ubuntu. Другую ОС (Windows, macOS) выбирают для специфичного тестирования (сборка под Windows, тесты для iOS).

**21. Основные ошибки CI/CD:**
Сложные монолитные jobs, отсутствие кэширования зависимостей, неправильные условия запуска.
*   **a. Почему нельзя хранить пароли в YAML?** YAML файлы хранятся в репозитории, доступны всем, у кого есть доступ к коду (и в истории коммитов).
*   **b. Как использовать secrets?** В настройках репозитория (Settings -> Secrets and variables -> Actions) создаются секреты. В workflow они используются как `${{ secrets.API_KEY }}`.

---
### **РАЗДЕЛ 4: LLM И PROMPT ENGINEERING**

**22. LLM за 15 секунд:**
Большая языковая модель — ИИ, обученный на огромных текстах, предсказывающий следующее слово/токен, способный генерировать связный текст и решать задачи.
*   **a. Tokenization:** Разбиение текста на токены (слова, части слов, символы). Важно, так как LLM работают с токенами, от токенизации зависят качество обработки и стоимость.
*   **b. Как генерирует текст?** Итеративно предсказывает следующий наиболее вероятный токен на основе контекста (промпта и уже сгенерированного текста).
*   **c. Как обучается?** Самообучение (self-supervised learning) на предсказании скрытых слов в текстах, затем дообучение с учителем (SFT) и reinforcement learning from human feedback (RLHF).

**23. Контекстное окно (context window):**
Максимальное количество токенов (входных + выходных), которое модель может обработать за один раз.
*   **a. Типичный размер и где узнать?** От 4K (старые GPT-3) до 128K-1M+ (Claude 3, GPT-4 Turbo). Указывается в документации к модели.
*   **b. Как обработать длинный текст?** Разбить на чанки, использовать методы RAG (извлечение релевантных частей), или модели с увеличенным контекстом.

**24. Prompt Engineering:**
Искусство и техника формулировки запросов (промптов) к LLM для получения точных и полезных результатов.
*   **a. Почему искусство?** Нет строгих научных правил. Результат зависит от формулировок, порядка, примеров, креативности. Это эмпирический процесс проб и ошибок.

**25. Отличие zero-shot от few-shot:**
*   **Zero-shot:** Задача ставится без примеров. "Переведи 'Hello' на французский".
*   **Few-shot:** В промпт дается несколько примеров "вход-выход". "Привет -> Bonjour, Пока -> Au revoir. Как будет 'Спасибо'?".
*   **a. Chain-of-Thought (CoT):** Промптинг, где у LLM просят "рассуждать шаг за шагом". Используют для сложных логических и математических задач.

**26. Параметр temperature:**
Параметр, контролирующий **случайность/креативность** вывода. Низкая (0.1-0.3) — детерминированные, фактологические ответы. Высокая (0.7-1.0) — разнообразные, креативные.
*   **a. Влияние на качество:** Слишком высокая — ответы становятся бессвязными, слишком низкая — шаблонными, могут зацикливаться. Для задач с одним верным ответом (код, факты) — низкая. Для творческих — высокая.

**27. Идеальный промпт:**
Ясный, конкретный, с контекстом и структурой.
*   **a. Принципы:** Роль (Act as a...), Задача, Контекст, Формат вывода, Ограничения.
*   **b. Пример для суммирования:** "Ты — эксперт по конспектированию. Суммаризируй предоставленную статью, выделив 3-5 ключевых тезисов. Ответ предоставь в виде маркированного списка. Статья: [ТЕКСТ]"
*   **c. Мой пример:** "Напиши функцию Python `parse_email(text)`, которая извлекает адрес электронной почты из строки `text`. Если email не найден, возвращай `None`. Не пиши пояснений, только код функции."

**28. Оптимизация стоимости LLM API:**
*   **a. Как оптимизировать?** Использовать более дешевые модели для простых задач, сжимать промпты, кэшировать ответы, ограничивать `max_tokens`.
*   **b. Обработка вывода:** Всегда валидировать и санитизировать вывод LLM (парсить JSON, проверять форматы, ловить исключения). Не доверять слепо.
*   **c. Просить форматировать:** Явно указывать: "Ответь в формате JSON с ключами `name` и `score`", "Выведи таблицу в Markdown".

**29. Почему prompt injection опасна?**
Злоумышленник может ввести текст в пользовательский запрос, который "переопределит" исходный системный промпт, заставив LLM выполнить вредоносные инструкции (раскрыть системный промпт, нарушить политики).
*   **a. Зачем guard rails?** Чтобы фильтровать ввод/вывод модели на предмет нежелательного контента (токсичность, личные данные), предотвращать инъекции и удерживать LLM в рамках предназначения приложения.

**30. Как оценить качество ответов LLM?**
По релевантности, точности (фактчекинг), полноте, связности и соответствию инструкциям. Использовать человеческую оценку, эталонные ответы (ground truth), автоматические метрики (BLEU, ROUGE).
*   **a. Почему нельзя доверять на 100%?** LLM могут быть абсолютно уверены в своих **галлюцинациях** (выдуманных фактах, коде). Уверенность не коррелирует с точностью.

**31. Главное ограничение LLM:**
**Отсутствие истинного понимания и рассуждения.** Они работают на статистических корреляциях в данных, а не на логике или знании мира.
*   **a. Может ли отказать?** Сама по себе — редко. Но система на ее основе может быть запрограммирована на отказ ("Я не могу ответить на этот вопрос").
*   **b. Почему "черный ящик"?** Сложно понять, как именно модель пришла к конкретному выводу из-за огромного числа параметров и сложных преобразований.

**32. System prompt (system role):**
Используется для задания **постоянного контекста, личности (persona) и глобальных инструкций** модели (например, в OpenAI Chat API).
*   **a. Persona:** Указание роли, которую должна играть модель: "Ты — опытный репетитор по математике, который объясняет понятным языком".
*   **b. Как справиться с bias?** Явно указывать в системном промпте требования к беспристрастности, предоставлять сбалансированные данные в контексте, пост-обрабатывать вывод.

**33. Что такое RAG?**
Архитектура, которая сочетает **поиск (retrieval)** релевантной информации из внешних источников (база знаний, документы) с **генерацией (generation)** ответа LLM на основе этой информации.
*   **a. Когда использовать?** Когда LLM нужны актуальные, специфичные или приватные данные, которых не было в ее обучающем наборе.
*   **b. Embedding и поиск:** Текст преобразуется в вектор чисел (embedding) с помощью модели. Похожие тексты имеют близкие векторы. Поиск — это нахождение векторов документов, наиболее близких к вектору запроса.

---
### **РАЗДЕЛ 5: MARKDOWN И MERMAID**

**34. Что такое Markdown?**
Упрощенный язык разметки для форматирования текста, который легко читается и конвертируется в HTML.
*   **a. Популярность vs Word:** Текстовый формат, версионируется в Git, легко diff'ится, универсален, отлично для документации в коде.
*   **b. Применения:** `README.md`, документация в репозиториях, issues и PR в GitHub, заметки (Obsidian, Notion), статические сайты (через генераторы).

**35. Создание в Markdown:**
*   **a. Заголовок 1:** `# Заголовок` или `Заголовок` + `===` под ним.
*   **b. Жирный:** `**жирный**` или `__жирный__`.
*   **c. Ссылка:** `[текст](https://example.com)`.
*   **d. Блок кода:** \```язык` на отдельной строке, затем код, затем \```.
*   **e. Таблица:**
    ```
    | Заголовок 1 | Заголовок 2 |
    |-------------|-------------|
    | Данные 1    | Данные 2    |
    ```

**36. Что такое Mermaid?**
Инструмент для создания диаграмм и визуализаций с помощью текстового описания на языке разметки.
*   **a. Типы диаграмм:** Flowchart (блок-схема), Sequence diagram, Class diagram, Gantt chart, Pie chart, State diagram.
*   **b. Польза в документации:** Позволяет хранить диаграммы как код, версионировать их, легко обновлять.
*   **c. Почему лучше PNG?** Текстовая форма: diff'ится в Git, легко редактируется, не размывается, занимает мало места, доступна для скринридеров.

**37. Встроить Mermaid в README:**
Обрамить код диаграммы тройными обратными кавычками с указанием `mermaid`.
    \```mermaid
    graph TD;
        A-->B;
    \```
*   **a. Пример class diagram:** Для документирования структуры классов в ООП-проекте, показа отношений наследования, композиции.

**38. Flowchart vs Sequence diagram:**
*   **Flowchart:** Для описания **процесса, алгоритма, потока управления** (что за чем следует).
*   **Sequence diagram:** Для описания **взаимодействия объектов/компонентов во времени** (кто кому и когда отправляет сообщение).
*   **a. Простая блок-схема:**
    \```mermaid
    graph TD
        Start[Начало] --> Input{Ввод данных};
        Input -->|Да| Process[Обработка];
        Input -->|Нет| Error[Ошибка];
        Process --> End[Конец];
    \```
*   **b. `TD` в Mermaid:** Top-Down (сверху вниз) — направление расположения узлов в графе. Альтернативы: `LR` (Left-Right, слева направо), `RL`, `BT`.

---
### **РАЗДЕЛ 6: ВАЙБ-КОДИНГ И COPILOT**

**39. Инструменты для вайб-кодинга (vibe-coding, потоковая разработка с AI):**
GitHub Copilot, Cursor, Tabnine, Codeium, ChatGPT (для проектирования).
*   **a. Разница Copilot и Codex:** Codex — это модель OpenAI, специализированная для кода, лежащая в основе ранних версий Copilot. Copilot — это готовый продукт (плагин для IDE) от GitHub, использующий улучшенные версии подобных моделей.
*   **b. Модель Copilot:** Изначально Codex (дочка GPT-3), сейчас вероятно используют собственные или доработанные модели (возможно, на базе GPT-4).

**40. Code review с AI-кодом:**
**Критически важен.** AI может генерировать уязвимый, неэффективный, некорректный или скопированный код.
*   **a. Три риска:** 1) **Безопасность:** внедрение уязвимостей. 2) **Лицензии:** копирование GPL-кода. 3) **Качество:** неоптимальные или ошибочные решения.
*   **b. Проверка качества:** Ручной review, запуск тестов, статический анализ (linters), проверка на плагиат.

**41. Комментарий для генерации кода:**
Быть максимально конкретным: указать вход, выход, обработку ошибок, стиль.
*   **a. Попросить функцию:** "Напиши функцию на Python, которая принимает список чисел и возвращает новый список, содержащий только четные числа, умноженные на 2. Назови функцию `process_even_numbers`. Добавь type hints."
*   **b. Функция сортировки:** "Напиши на JavaScript функцию `sortUsersByAge(users)`, которая принимает массив объектов `user` с полями `name` и `age` и возвращает массив, отсортированный по возрасту (`age`) по возрастанию. Используй встроенный метод `sort`."

**42. Ошибка в сгенерированном коде:**
1. Проанализировать ошибку. 2. Уточнить промпт, добавив контекст об ошибке. 3. Попросить LLM исправить конкретную проблему.
*   **a. Помощь в отладке:** Можно скопировать код и сообщение об ошибке в LLM: "Вот мой код [КОД] и ошибка [ОШИБКА]. Объясни, в чем проблема и как исправить?"

**43. Workflow человек + AI:**
1. Человек ставит задачу (промпт высокого уровня). 2. AI генерирует черновик кода/плана. 3. Человек ревьюит, тестирует, уточняет. 4. AI дорабатывает. 5. Цикл повторяется.
*   **a. Итеративный prompt engineering:** Постепенное уточнение и улучшение промптов на основе предыдущих ответов модели — ключ к получению точного результата.

**44. Сложные языки для LLM:**
Языки с маленькими сообществами и кодобазами (нишевые DSL, устаревшие), а также очень низкоуровневые (Assembler). Сложности с экосистемой и фреймворками (например, SAP ABAP, 1C).
*   **a. Может ли рефакторить код 1C?** Скорее всего, плохо, из-за малого объема обучающих данных по 1C в открытом доступе.

**45. Объединение Git, CI/CD и AI:**
*   **AI генерирует код ->** код пушится в ветку Git -> **CI/CD пайплайн** автоматически запускает тесты и линтеры -> после ревью мержится.
*   **a. LLM в CI/CD:** Например, в GitHub Actions можно добавить шаг, где LLM (через API) генерирует release notes на основе коммитов или комментирует PR.

**46. Студенту и AI:**
*   **a. Зачем понимать базовые концепции?** AI — инструмент, а не замена. Без фундамента нельзя оценить корректность его вывода, выбрать правильную архитектуру, дебажить.
*   **b. Что AI делает лучше/хуже:**
    *   **Лучше:** Генерация шаблонного кода, документация, объяснение концепций, поиск багов в синтаксисе.
    *   **Требует человека:** Архитектурные решения, сложная бизнес-логика, критические security-моменты, оценка "качества" кода в контексте проекта.

**47. Качество AI-разработки:**
*   **a. Главные вызовы:** Контроль качества, безопасность, соблюдение лицензий, зависимость от провайдера, "интеллектуальная лень" разработчика.
*   **b. Инструменты автоматизации с AI:** AI-линтеры и анализаторы безопасности (например, Copilot Scanning), инструменты для автотестов, AI-ассистенты для ревью (например, CodiumAI PR-Agent).

---
### **РАЗДЕЛ 7: АГЕНТНЫЕ СИСТЕМЫ**

**48. Агентная система vs Chatbot:**
Chatbot пассивно реагирует на запросы. **AI Agent** — это автономная система, которая **сама ставит цели, планирует и выполняет действия** (используя инструменты/Tools) для их достижения.
*   **a. Цикл работы:** **Восприятие (Perceive)** -> **Планирование (Plan)** -> **Действие (Act)** -> **Наблюдение результата (Observe)** -> повтор.
*   **b. Типы агентов:** Реактивные, с планированием (planning), с рефлексией (reflective), многоагентные системы.
*   **c. Пример в приложении:** Агент для автоматической категоризации и ответа на тикет поддержки: читает тикет, определяет тему, ищет решение в Базе Знаний, формирует ответ.

**49. Итеративная работа агента:**
Агент разбивает большую цель на подзадачи и решает их шаг за шагом, используя вывод предыдущего шага как контекст для следующего.
*   **a. Две обязательные части:** **Ядро (LLM + промпт-логика)** и **Инструменты (Tools/API)**, которые агент может вызывать (поиск в интернете, выполнение кода, запрос к БД).
*   **b. Зачем память (state)?** Чтобы помнить историю взаимодействия, цели, промежуточные результаты, контекст разговора с пользователем.

**50. Gamma как агентная система:**
Предположительно, система (по аналогии с Alphafold) для работы с документами, где разные агенты выполняют роли: парсер, аналитик, суммаризатор, генератор.
*   **a. Агенты для своей Gamma:** Агент **Парсер** (извлекает текст/данные), Агент **Анализатор** (структурирует, находит сущности), Агент **Суммаризатор**, Агент **Ответчик** (Q&A).
*   **b. Обработка неструктурированных данных:** Агенты используют LLM для понимания смысла, извлечения сущностей и преобразования в структурированный вид (JSON, таблица).
*   **c. Возможности Gamma:** Автоматическое чтение, анализ, суммаризация, сравнение документов, извлечение контрактов, ответы на вопросы по документам.

**51. Пример агентной системы в разработке:**
Автономный агент для поиска и исправления багов: анализирует лог ошибок, ищет соответствующий код, генерирует и тестирует потенциальный фикс, создает PR.
*   **a. Почему важны?** Автоматизируют сложные, многошаговые процессы, требующие рассуждений и использования инструментов, выходящие за рамки одного LLM-запроса.
*   **b. Как контролировать?**
    *   **Человек в петле (Human-in-the-loop):** Запрос подтверждения на критичные действия.
    *   **Строгие правила (Guardrails):** Ограничение набора доступных инструментов и их параметров.
    *   **Мониторинг и откат:** Логирование всех действий, возможность отката.

---

Вы можете скопировать этот текст и сохранить его как `answers.txt` или `answers.md`.
